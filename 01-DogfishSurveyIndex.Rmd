---
title: "DogfishSurveySummary"
author: "davidson"
date: "7/17/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## DOG comparison wih HBLL survey

Goal: 
Eliminate the dogfish fall survey and generate an dogfish index using survey data from the HBLL Inside survey. 

However, 
- [1]	HBLL survey runs in the summer and dogfish have seasonal movements and there is a concern we are not capturing the larger females. 
- [2]	The HBLL survey has a shallower depth strata than the dogfish survey.
- [3]	The HBLL survey uses different bait and hooks. 
- [4]	The dogfish survey was a fixed station with 4 depth strata. 

Therefore, 
- [1]	Continue Dogfish (fall) survey until a full analytical comparison of the two time series is completed. 
- [2]	Complete seasonal and gear/bait comparative work. 

Here I ask:
- [1] Is the size distribution between the HBLL and DOG (fall Dogfish survey) different?
- [3] Is the relative index of abundance estimated to be the same across the two surveys?
- [2] Is the decline of maturity classes estimated to be similar across the two surveys?

Questions to address:
- [1]	Do extra depths need to be added to the HBLL survey to capture an sex/length composition to create an index? Evaluate data as surveys are completed to understand the catch size and sex distribution. Seems that dogfish are shallower in summer and itâ€™s possible that we are capturing the same size and sex distribution at shallower depths. However, we only have data from three sites and the results are variable. Dogfish move deeper in the later months. Also potential for differences in the catchability in individuals between seasons?
- [2]	Are there differences in the catchability of dogfish between the HBLL and Dogfish surveys owing to bait/gear)? 
- [3]	What is the calibration coefficient between the gear types? seasons?


```{r library}
library(gfdata)
library(gfplot)
library(tidyverse)
library(here)
library(sdmTMB)
library(sf)
library(sp)
```

There are four DOG surveys in gfbio: `x$SURVEY_SERIES_DESC[x$SURVEY_SERIES_ID %in% c(48, 76, 92, 93)]`. These surveys include the two surveys that ran in 1986 and 1989 with J-hooks (ssid = 92); the comparison surveys completed in 2004, 2019, 2022 (ssid = 48), and the surveys completed with circle hooks in 2005, 2008, 2011, 2014, 2019.   

However, `get_survey_sets` only pulls data from the 2019 survey that includes the DOG survey and the HBLL comparative work `unique(sets$year); unique(sets$survey_series_desc)`). 

Details of the comparative surveys. 2019 HBLL with DOG circle hooks run at x stations. 2022 comparative work included HBLL and DOG circle hooks, same line, run at x sites. 2004 work was completed during the DOG survey and included fishing sits with J-hooks and once with circle hooks on the same set (ref:). 

```{r data, eval = TRUE}
x <- get_ssids()
x$SURVEY_SERIES_DESC[x$SURVEY_SERIES_ID %in% c(48, 76, 92, 93)]
sets <- get_survey_sets(species = "north pacific spiny dogfish", ssid = c(48, 76, 92, 93))
# saveRDS(sets,  "C:/Dogfish surveys 2022/output/dogfish_sets.rds")
sets <- readRDS("output/dogfish_sets.rds")

```


Create an index using the HBLL and DOG surveys. Load DOGFISH survey data from Dogfish_data_pull.R 
```{r DOGsurveydata }

dog <- readRDS("output/dogfishdata.rds")

#years <- seq(min(dog$year), max(dog$year), 1)
years <- unique(dog$year)

dog$logbot_depth <- log(dog$depth_m)
ggplot(dog, aes(longitude, latitude, colour = location)) +
  geom_point() +
  facet_wrap(~year)
dog <- add_utm_columns(dog,
  ll_names = c("longitude", "latitude"), units = "km",
  utm_crs = 32609
)
mean_logbot_depth <- mean(dog$logbot_depth, na.rm = TRUE)
dog$logbot_depth_cent <- dog$logbot_depth - mean_logbot_depth
dog <- filter(dog, !is.na(depth_m))
dog <- filter(dog, !is.na(julian))
dog <- filter(dog, !is.na(dogfish_count))
dog$offset <- log(dog$lglsp_hook_count)

dog$locationf <- as.factor(dog$location)
x <- dog |>
  group_by(year, location) |>
  tally()
# dog <- dog |>
#   group_by(year, location) |>
#   mutate(tally = n()) |>
#   filter(tally >=3)
```


Create grid
```{r grid, eval = FALSE}

shelf_SOG <- st_read("data", "SOG_polygon")
plot(st_geometry(shelf_SOG), col = "red")
shelf2 <- st_transform(shelf_SOG, "+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs") #+proj=utm +zone=9 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs
plot(st_geometry(shelf2), col = "red")

# Create grid that covers the BC coast polygon/jurisdiction.
grid_spacing <- 2000 # 2 X 2

polygony <- st_make_grid(shelf2, square = T, cellsize = c(grid_spacing, grid_spacing)) %>%
  st_sf() %>%
  mutate(cell_ID = row_number())
plot(st_geometry(polygony))

center <- st_centroid(polygony)
# grid_extent <- st_intersection(st_geometry(shelf2), st_geometry(polygony))
grid_extent <- st_intersection(shelf2, polygony)
plot(st_geometry(grid_extent))
center2 <- st_centroid(grid_extent)

test <- st_sf(grid_extent)
test_center <- st_sf(center2)
plot(st_geometry(grid_extent))
plot(st_geometry(test_center), add = TRUE)

test$area_km <- st_area(test) / 1000000 # m to km
plot(st_geometry(test))
plot(test_center, add = TRUE, pch = 21)
st_write(test, "output/PredictionGrid_SOG.shp", append = FALSE)
st_write(test_center, "output/PredictionGridCentres_SOG.shp", append = FALSE)


# citation("PBSmapping")
library(PBSmapping)
data(bcBathymetry) # bathymetry
contour(bcBathymetry$x, bcBathymetry$y, bcBathymetry$z, col = "pink", method = "edge", vfont = c("sans serif", "plain"))

cont <- contourLines(bcBathymetry$x, bcBathymetry$y, bcBathymetry$z, nlevels = 1000)
clines <- maptools::ContourLines2SLDF(cont)
clines@data[["level"]]
lines <- st_as_sf(clines) # make sf object
plot(lines)

st_crs(lines) <- sp::CRS("+proj=longlat")
c.linesproj <- st_transform(lines, crs = "+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs")
plot(c.linesproj)
gridarea <- st_intersection(st_geometry(shelf2), st_geometry(c.linesproj)) %>% st_sfc(crs = "+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs")
plot(gridarea)

gridarea2 <- st_collection_extract(gridarea, "LINESTRING")
st_write(gridarea2, "output/Contours_SOG.shp", append = FALSE)
plot(gridarea2)


# Overlap the grid and the contour to get depth per grid point
bathymetry <- st_read("output/Contours_SOG.shp")
prediction_grid <- st_read("output/PredictionGrid_SOG.shp")
plot(st_geometry(prediction_grid))
prediction_center <- st_read("output/PredictionGridCentres_SOG.shp")
glimpse(prediction_center)

centerdf <- do.call(rbind, st_geometry(prediction_center)) %>%
  as_tibble() %>%
  setNames(c("X", "Y"))
centerdf$value <- seq(1, nrow(centerdf))

# lat and longs for each year to predict on
centerdf2 <- expand.grid(unique(centerdf$value), years)
names(centerdf2) <- c("value", "year")
centerdf3 <- left_join(centerdf2, centerdf, by = c("value" = "value"))
unique(centerdf3$year)

## Create the clipping polygon
maxlat <- max(centerdf$Y) # Northing, lat
minlat <- min(centerdf$Y) # northing, lat
maxlon <- max(centerdf$X) # Easting, long
minlon <- min(centerdf$X) # easting, long

b <- marmap::getNOAA.bathy(lon1 = -150, lon2 = -110, lat1 = 30, lat2 = 70, resolution = 1)

# convert center utms to lat and longs
centerlatlon <- as.data.frame(centerdf) # y is lat, x is lon
coordinates(centerlatlon) <- c("X", "Y") # c( "X", "Y")
proj4string(centerlatlon) <- CRS("+proj=utm +zone=9 +datum=WGS84")
centertrans <- spTransform(centerlatlon, CRS = "+proj=longlat + datum=WGS84")
centertrans2 <- as.data.frame(coordinates(centertrans))

names(centertrans2) <- c("lon", "lat")
# names(centerdf3) <- c("value", "year", "UTM.lon", "UTM.lat")
centertrans3 <- cbind(centerdf, centertrans2)
names(centertrans3) <- c("UTM.lon.m", "UTM.lat.m", "value", "lon", "lat")
depthpoints_center <- marmap::get.depth(b, centertrans3[, c("lon", "lat")], locator = FALSE)
depthpoints_center2 <- depthpoints_center[!duplicated(depthpoints_center), ] # not sure why I had this...maybe to calculate mean and sd
centertrans4 <- inner_join(depthpoints_center2, centertrans3, by = c("lat" = "lat", "lon" = "lon"))


# see which point are the ones that have positive depths
depth_predictiongrid <- filter(centertrans4, depth > -4)
max(depth_predictiongrid$depth)
plot(depth_predictiongrid$lon, depth_predictiongrid$lat) # prediction grid points with depth > 0
dim(depth_predictiongrid)[1] # num pred. grid points with depth > 0

# erase positive depth points
# the range of deptsh surveyed by IPHC and Tralw and Longline is 9 - 1308 meters.
centertrans5 <- filter(centertrans4, depth < -4)
glimpse(centertrans5) # value is grid cell id
plot(centertrans5$lon, centertrans5$lat) # prediction grid points with depth > 0

names(centertrans5)[3] <- "depth_m"
centertrans5$depth_m_neg <- centertrans5$depth_m
centertrans5$depth_m <- centertrans5$depth_m_neg * -1
centertrans5$logdepth <- log10(centertrans5$depth_m)
saveRDS(centertrans5, "output/predictiongrid_bccoast_SOG.rds")
```

Load prediction grid
```{r predictiongridload}

grid <- readRDS("output/predictiongrid_bccoast_SOG.rds")
ggplot(grid, aes(UTM.lon.m, UTM.lat.m, colour = depth_m)) +
  geom_point()
grid <- grid |> filter(UTM.lon.m > 750000)
grid$offset <- log(1)
max <- as.numeric(max(dog$depth_m))
min <- as.numeric(min(dog$depth_m))
grid <- grid |> filter(depth_m <= max)
grid <- grid |> filter(depth_m >= min)
grid$logbot_depth <- log(grid$depth_m)
grid$X <- grid$UTM.lon.m / 1000
grid$Y <- grid$UTM.lat.m / 1000
sort(unique(grid$year))
ggplot(grid, aes(UTM.lon.m, UTM.lat.m, colour = depth_m)) +
  geom_point()
grid$julian <- 287
grid$juliansmall = 0
grid$logbot_depth_cent <- grid$logbot_depth - mean_logbot_depth

range(dog$Y)
range(dog$X)

grid <- grid |> filter(X <= 915.7025 )
grid <- grid |> filter(Y <=5545.813 )

grid <- grid |> filter(X >= 777.9039 )
grid <- grid |> filter(Y >= 5433.850 )

```

Create mesh
```{r}

mesh <- make_mesh(dog, xy_cols = c("X", "Y"), cutoff = 20)

plot(mesh$mesh, asp = 1, main = "")
points(grid$X, grid$Y, col = "blue")
points(dog$X, dog$Y, col = "red")

fourth_root_power_trans <- function() {
  scales::trans_new(
    name = "fourth root power",
    transform = function(x) ifelse(x > 0, x^0.25, -(-x)^0.25),
    inverse = function(x) ifelse(x > 0, x^4, -(-x)^4),
    domain = c(-Inf, Inf)
  )
}

ggplot() +
  inlabru::gg(mesh$mesh) +
  coord_fixed() +
  geom_point(aes(X, Y), data = dog, alpha = 0.2, size = 0.5) +
  geom_point(aes(X, Y, colour = dogfish_count, size = dogfish_count),
    data = filter(dog, dogfish_count > 0)
  ) +
  facet_wrap(~year) +
  scale_color_viridis_c(trans = "fourth_root_power")
```


```{r indexmodelDOG}

#dog$juliansmall <- dog$julian - min(dog$julian)
#dog <- filter(dog, year > 2000)

unique(sort(dog$year))
dog |>
  ggplot() +
  geom_point(aes(year, dogfish_count)) 

sum(dog$dogfish_count ==0) # 

dog |>
  group_by(year) |>
  mutate(sum = sum(dogfish_count)) |>
  ggplot() +
  geom_point(aes(year, sum)) +
  geom_line(aes(year, sum))

dog |>
  group_by(year) |>
  mutate(cpue = sum(dogfish_count) / sum(lglsp_hook_count)) |>
  ggplot() +
  geom_point(aes(year, cpue)) +
  geom_line(aes(year, cpue))
range(dog$dogfish_count)
range(dog$offset)
range(dog$year)
dog <- droplevels(dog)
dog <- dog |> 
  mutate(dogfish_count = ifelse(year %in% c(1986, 1989), dogfish_count * 1.6, dogfish_count))
dog$smalloffset <- log(dog$hook_count/10)

t <- filter(dog, gear_type)
hist(dog$offset)

#do the calibration in the model (2004) 
#include every row of data gets a gear type
#colum of gear type
#include geartype in the model 
#different catcability for each geartype
#a ratio of catchability
#predict on a geartype.....
#for prediction grid
gfplot::dogfish_grid
ggplot(dogfish_grid$grid, aes(X, Y )) + geom_point()
ggplot(dog, aes(X, Y)) + geom_point()

mind <- sdmTMB(
  formula = dogfish_count ~  1 + as.factor(year) + juliansmall, # + logbot_depth_cent + I(logbot_depth_cent^2) + juliansmall + locationf ,
  offset = dog$offset,
  data = dog,
  mesh = mesh, 
  spatiotemporal = "off", 
  #spatiotemporal = FALSE, 
  #spatiotemporal = "IID",
  time = "year",
  #extra_time = c(2006, 2007, 2009, 2010, 2012, 2013, 2015, 2016, 2017, 2018),
  silent = FALSE,
  family = Gamma(link = "log"), #compare posson family (count dis with no dispersion, mean = variance), create new column that is unique for each observation, then create a random effect , same number of parameter, can compare AIC with Nbinom2, ( 1|obs) creates one sd allows for more variability dispersion in he data, 
  spatial = TRUE)

sanity(mind)
mind$sd_report
saveRDS(mind, "output/DOGmodel.rds")

#years <- seq(min(dog$year), max(dog$year), 1)
years <- unique(dog$year)
grid <- grid |> select(-year) |> distinct()
grid <- purrr::map_dfr(as.numeric(years), ~ tibble(dogfish_grid$grid, year = .x))
grid$juliansmall <- 0
grid$offset <- 0
#grid$locationf <- "Cape Mudge"
pred <- predict(mind, newdata = grid, return_tmb_object = TRUE)
index <- get_index(pred, area = 4, bias_correct = TRUE)

# ggplot(grid, aes(X, Y)) + geom_point()
# 
ggplot(index, aes(year, est)) +
  geom_line(col = "#8D9999") +
  geom_point(col = "#8D9999") +
   geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4, fill = "#8D9999") +
  theme_classic()

```
